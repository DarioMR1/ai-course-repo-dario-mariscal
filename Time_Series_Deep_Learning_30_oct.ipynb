{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series with Deep Learning: CNN + LSTM\n",
    "\n",
    "**Date:** 30 October 2025\n",
    "\n",
    "**Topic:** Advanced time series forecasting using Convolutional Neural Networks and Long Short-Term Memory networks\n",
    "\n",
    "This notebook covers:\n",
    "1. Time Series Data Preparation for Deep Learning\n",
    "2. Supervised Window Creation for Sequential Data\n",
    "3. 1D Convolutional Neural Networks for Time Series\n",
    "4. LSTM Networks for Sequential Pattern Recognition\n",
    "5. Hybrid CNN-LSTM Architecture\n",
    "6. Model Comparison and Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, LSTM, TimeDistributed\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\nsns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Synthetic Sales Dataset Generation\n",
    "\n",
    "### 2.1 Creating Store Sales Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic sales data similar to store-item sales\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate 5 years of daily sales data\n",
    "n_days = 5 * 365  # 5 years\n",
    "dates = pd.date_range(start='2018-01-01', periods=n_days, freq='D')\n",
    "\n",
    "# Base sales pattern with trend and seasonality\n",
    "t = np.arange(n_days)\n",
    "trend = 0.001 * t + 10  # Slight upward trend\n",
    "seasonal = 3 * np.sin(2 * np.pi * t / 365)  # Annual seasonality\n",
    "weekly = 1.5 * np.sin(2 * np.pi * t / 7)    # Weekly pattern\n",
    "noise = np.random.normal(0, 1.5, n_days)     # Random noise\n",
    "\n",
    "# Combine components to create sales data\n",
    "sales = trend + seasonal + weekly + noise\n",
    "sales = np.maximum(sales, 1)  # Ensure positive sales\n",
    "sales = np.round(sales).astype(int)  # Integer sales values\n",
    "\n",
    "# Create DataFrame\n",
    "sales_data = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'store': 1,\n",
    "    'item': 1,\n",
    "    'sales': sales\n",
    "})\n",
    "\n",
    "print(f\"Generated Sales Dataset:\")\n",
    "print(f\"Total days: {len(sales_data)}\")\n",
    "print(f\"Date range: {sales_data['date'].min()} to {sales_data['date'].max()}\")\n",
    "print(f\"Sales statistics:\")\n",
    "print(f\"  Mean: {sales_data['sales'].mean():.2f}\")\n",
    "print(f\"  Std: {sales_data['sales'].std():.2f}\")\n",
    "print(f\"  Min: {sales_data['sales'].min()}\")\n",
    "print(f\"  Max: {sales_data['sales'].max()}\")\n",
    "\n",
    "display(sales_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Sales Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive sales visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Complete time series\n",
    "ax1.plot(sales_data['date'], sales_data['sales'], alpha=0.7, linewidth=1)\n",
    "ax1.set_title('Complete Sales Time Series (5 Years)', fontsize=14)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Sales')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: First year detail\n",
    "first_year = sales_data[sales_data['date'].dt.year == 2018]\n",
    "ax2.plot(first_year['date'], first_year['sales'], color='orange', linewidth=1.5)\n",
    "ax2.set_title('Sales Pattern - First Year Detail', fontsize=14)\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_ylabel('Sales')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Sales distribution\n",
    "ax3.hist(sales_data['sales'], bins=30, alpha=0.7, color='green', edgecolor='black')\n",
    "ax3.set_title('Sales Distribution', fontsize=14)\n",
    "ax3.set_xlabel('Sales Volume')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Monthly average pattern\n",
    "monthly_avg = sales_data.groupby(sales_data['date'].dt.month)['sales'].mean()\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "ax4.bar(range(1, 13), monthly_avg.values, alpha=0.8, color='purple')\n",
    "ax4.set_title('Average Sales by Month', fontsize=14)\n",
    "ax4.set_xlabel('Month')\n",
    "ax4.set_ylabel('Average Sales')\n",
    "ax4.set_xticks(range(1, 13))\n",
    "ax4.set_xticklabels(months)\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing for Deep Learning\n",
    "\n",
    "### 3.1 Supervised Learning Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_supervised_windows(series: np.ndarray, window: int, lag: int = 1):\n",
    "    \"\"\"\n",
    "    Convert time series to supervised learning format\n",
    "    \n",
    "    Parameters:\n",
    "    - series: 1D array of time series values\n",
    "    - window: number of past time steps to use as features\n",
    "    - lag: number of steps ahead to predict (default: 1)\n",
    "    \n",
    "    Returns:\n",
    "    - X: feature matrix of shape (samples, window+1)\n",
    "    - y: target vector of shape (samples,)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(window, len(series) - lag + 1):\n",
    "        X.append(series[i-window:i])  # Features: past 'window' values\n",
    "        y.append(series[i + lag - 1])  # Target: value 'lag' steps ahead\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Configuration for supervised learning\n",
    "window_size = 29  # Use 29 days to predict next day\n",
    "forecast_lag = 1   # Predict 1 day ahead\n",
    "\n",
    "# Convert to supervised format\n",
    "X, y = make_supervised_windows(sales_data['sales'].values, window_size, forecast_lag)\n",
    "\n",
    "print(f\"Supervised Learning Data Shape:\")\n",
    "print(f\"Features (X): {X.shape}\")\n",
    "print(f\"Targets (y): {y.shape}\")\n",
    "print(f\"Total samples: {len(X)}\")\n",
    "print(f\"Window size: {window_size} days\")\n",
    "print(f\"Prediction lag: {forecast_lag} day(s)\")\n",
    "\n",
    "# Display sample data\n",
    "print(f\"\\nSample data:\")\n",
    "print(f\"First sample features (X[0]): {X[0][:10]}... (showing first 10 values)\")\n",
    "print(f\"First sample target (y[0]): {y[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train-Test Split and Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal train-test split (80-20)\n",
    "split_point = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_point], X[split_point:]\n",
    "y_train, y_test = y[:split_point], y[split_point:]\n",
    "\n",
    "print(f\"Data Split:\")\n",
    "print(f\"Training samples: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Testing samples: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Feature scaling for better neural network performance\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "# Fit scalers on training data only\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
    "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).ravel()\n",
    "\n",
    "print(f\"\\nScaled Data Statistics:\")\n",
    "print(f\"X_train_scaled: mean={X_train_scaled.mean():.4f}, std={X_train_scaled.std():.4f}\")\n",
    "print(f\"y_train_scaled: mean={y_train_scaled.mean():.4f}, std={y_train_scaled.std():.4f}\")\n",
    "\n",
    "# Reshape for 3D input required by CNN and LSTM (samples, timesteps, features)\n",
    "X_train_3d = X_train_scaled.reshape((len(X_train_scaled), window_size, 1))\n",
    "X_test_3d = X_test_scaled.reshape((len(X_test_scaled), window_size, 1))\n",
    "\n",
    "print(f\"\\n3D Reshaped Data:\")\n",
    "print(f\"X_train_3d shape: {X_train_3d.shape}\")\n",
    "print(f\"X_test_3d shape: {X_test_3d.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model 1: 1D Convolutional Neural Network\n",
    "\n",
    "### 4.1 CNN Architecture Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build 1D CNN model for time series forecasting\n",
    "model_cnn = Sequential([\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(window_size, 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model_cnn.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "print(\"1D CNN Model Architecture:\")\n",
    "print(\"=\" * 50)\n",
    "model_cnn.summary()\n",
    "\n",
    "# Calculate total parameters\n",
    "total_params = model_cnn.count_params()\n",
    "print(f\"\\nTotal trainable parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 CNN Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CNN model\n",
    "print(\"Training 1D CNN Model...\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "history_cnn = model_cnn.fit(\n",
    "    X_train_3d, y_train_scaled,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    "    shuffle=False  # Important for time series to maintain temporal order\n",
    ")\n",
    "\n",
    "print(\"\\nCNN Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 CNN Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with CNN\n",
    "y_pred_cnn_scaled = model_cnn.predict(X_test_3d).ravel()\n",
    "y_pred_cnn = scaler_y.inverse_transform(y_pred_cnn_scaled.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Calculate performance metrics\n",
    "rmse_cnn = np.sqrt(mean_squared_error(y_test, y_pred_cnn))\n",
    "mae_cnn = mean_absolute_error(y_test, y_pred_cnn)\n",
    "mape_cnn = np.mean(np.abs((y_test - y_pred_cnn) / y_test)) * 100\n",
    "\n",
    "print(f\"CNN Model Performance:\")\n",
    "print(f\"RMSE: {rmse_cnn:.4f}\")\n",
    "print(f\"MAE:  {mae_cnn:.4f}\")\n",
    "print(f\"MAPE: {mape_cnn:.2f}%\")\n",
    "\n",
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(history_cnn.history['loss'], label='Training Loss')\n",
    "ax1.plot(history_cnn.history['val_loss'], label='Validation Loss')\n",
    "ax1.set_title('CNN Model Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss (MSE)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# MAE curves\n",
    "ax2.plot(history_cnn.history['mae'], label='Training MAE')\n",
    "ax2.plot(history_cnn.history['val_mae'], label='Validation MAE')\n",
    "ax2.set_title('CNN Model MAE')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('MAE')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model 2: Long Short-Term Memory (LSTM)\n",
    "\n",
    "### 5.1 LSTM Architecture Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM model for time series forecasting\n",
    "model_lstm = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(window_size, 1)),\n",
    "    LSTM(32, activation='tanh'),\n",
    "    Dense(25, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model_lstm.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "print(\"LSTM Model Architecture:\")\n",
    "print(\"=\" * 50)\n",
    "model_lstm.summary()\n",
    "\n",
    "# Calculate total parameters\n",
    "total_params_lstm = model_lstm.count_params()\n",
    "print(f\"\\nTotal trainable parameters: {total_params_lstm:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 LSTM Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM model\n",
    "print(\"Training LSTM Model...\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "history_lstm = model_lstm.fit(\n",
    "    X_train_3d, y_train_scaled,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(\"\\nLSTM Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 LSTM Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with LSTM\n",
    "y_pred_lstm_scaled = model_lstm.predict(X_test_3d).ravel()\n",
    "y_pred_lstm = scaler_y.inverse_transform(y_pred_lstm_scaled.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Calculate performance metrics\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_test, y_pred_lstm))\n",
    "mae_lstm = mean_absolute_error(y_test, y_pred_lstm)\n",
    "mape_lstm = np.mean(np.abs((y_test - y_pred_lstm) / y_test)) * 100\n",
    "\n",
    "print(f\"LSTM Model Performance:\")\n",
    "print(f\"RMSE: {rmse_lstm:.4f}\")\n",
    "print(f\"MAE:  {mae_lstm:.4f}\")\n",
    "print(f\"MAPE: {mape_lstm:.2f}%\")\n",
    "\n",
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(history_lstm.history['loss'], label='Training Loss', color='green')\n",
    "ax1.plot(history_lstm.history['val_loss'], label='Validation Loss', color='orange')\n",
    "ax1.set_title('LSTM Model Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss (MSE)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# MAE curves\n",
    "ax2.plot(history_lstm.history['mae'], label='Training MAE', color='green')\n",
    "ax2.plot(history_lstm.history['val_mae'], label='Validation MAE', color='orange')\n",
    "ax2.set_title('LSTM Model MAE')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('MAE')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model 3: Hybrid CNN-LSTM Architecture\n",
    "\n",
    "### 6.1 CNN-LSTM Design with TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for CNN-LSTM hybrid model\n",
    "# Reshape data into subsequences for TimeDistributed CNN\n",
    "n_subsequences = 5\n",
    "n_timesteps = window_size // n_subsequences\n",
    "\n",
    "# Ensure window_size is divisible by n_subsequences\n",
    "assert window_size % n_subsequences == 0, f\"Window size {window_size} must be divisible by {n_subsequences}\"\n",
    "\n",
    "print(f\"CNN-LSTM Data Reshaping:\")\n",
    "print(f\"Original shape: {X_train_3d.shape}\")\n",
    "print(f\"Number of subsequences: {n_subsequences}\")\n",
    "print(f\"Timesteps per subsequence: {n_timesteps}\")\n",
    "\n",
    "# Reshape for TimeDistributed CNN + LSTM\n",
    "X_train_4d = X_train_3d.reshape((len(X_train_3d), n_subsequences, n_timesteps, 1))\n",
    "X_test_4d = X_test_3d.reshape((len(X_test_3d), n_subsequences, n_timesteps, 1))\n",
    "\n",
    "print(f\"Reshaped to 4D:\")\n",
    "print(f\"X_train_4d shape: {X_train_4d.shape}\")\n",
    "print(f\"X_test_4d shape: {X_test_4d.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build hybrid CNN-LSTM model\n",
    "model_cnn_lstm = Sequential([\n",
    "    # TimeDistributed CNN layers to process each subsequence\n",
    "    TimeDistributed(Conv1D(filters=32, kernel_size=2, activation='relu'), \n",
    "                    input_shape=(n_subsequences, n_timesteps, 1)),\n",
    "    TimeDistributed(MaxPooling1D(pool_size=2)),\n",
    "    TimeDistributed(Flatten()),\n",
    "    \n",
    "    # LSTM layer to process sequence of CNN outputs\n",
    "    LSTM(32, activation='tanh'),\n",
    "    \n",
    "    # Dense layers for final prediction\n",
    "    Dense(25, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model_cnn_lstm.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "print(\"Hybrid CNN-LSTM Model Architecture:\")\n",
    "print(\"=\" * 50)\n",
    "model_cnn_lstm.summary()\n",
    "\n",
    "# Calculate total parameters\n",
    "total_params_hybrid = model_cnn_lstm.count_params()\n",
    "print(f\"\\nTotal trainable parameters: {total_params_hybrid:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 CNN-LSTM Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train hybrid CNN-LSTM model\n",
    "print(\"Training Hybrid CNN-LSTM Model...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "history_cnn_lstm = model_cnn_lstm.fit(\n",
    "    X_train_4d, y_train_scaled,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(\"\\nHybrid CNN-LSTM Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 CNN-LSTM Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with hybrid CNN-LSTM\n",
    "y_pred_hybrid_scaled = model_cnn_lstm.predict(X_test_4d).ravel()\n",
    "y_pred_hybrid = scaler_y.inverse_transform(y_pred_hybrid_scaled.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Calculate performance metrics\n",
    "rmse_hybrid = np.sqrt(mean_squared_error(y_test, y_pred_hybrid))\n",
    "mae_hybrid = mean_absolute_error(y_test, y_pred_hybrid)\n",
    "mape_hybrid = np.mean(np.abs((y_test - y_pred_hybrid) / y_test)) * 100\n",
    "\n",
    "print(f\"Hybrid CNN-LSTM Model Performance:\")\n",
    "print(f\"RMSE: {rmse_hybrid:.4f}\")\n",
    "print(f\"MAE:  {mae_hybrid:.4f}\")\n",
    "print(f\"MAPE: {mape_hybrid:.2f}%\")\n",
    "\n",
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(history_cnn_lstm.history['loss'], label='Training Loss', color='purple')\n",
    "ax1.plot(history_cnn_lstm.history['val_loss'], label='Validation Loss', color='red')\n",
    "ax1.set_title('CNN-LSTM Model Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss (MSE)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# MAE curves\n",
    "ax2.plot(history_cnn_lstm.history['mae'], label='Training MAE', color='purple')\n",
    "ax2.plot(history_cnn_lstm.history['val_mae'], label='Validation MAE', color='red')\n",
    "ax2.set_title('CNN-LSTM Model MAE')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('MAE')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison and Analysis\n",
    "\n",
    "### 7.1 Performance Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive performance comparison\n",
    "performance_data = {\n",
    "    'Model': ['1D CNN', 'LSTM', 'CNN-LSTM Hybrid'],\n",
    "    'RMSE': [rmse_cnn, rmse_lstm, rmse_hybrid],\n",
    "    'MAE': [mae_cnn, mae_lstm, mae_hybrid],\n",
    "    'MAPE (%)': [mape_cnn, mape_lstm, mape_hybrid],\n",
    "    'Parameters': [total_params, total_params_lstm, total_params_hybrid]\n",
    "}\n",
    "\n",
    "performance_df = pd.DataFrame(performance_data)\n",
    "performance_df = performance_df.round({'RMSE': 4, 'MAE': 4, 'MAPE (%)': 2})\n",
    "\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "display(performance_df)\n",
    "\n",
    "# Find best performing model\n",
    "best_model_idx = performance_df['RMSE'].idxmin()\n",
    "best_model = performance_df.loc[best_model_idx, 'Model']\n",
    "best_rmse = performance_df.loc[best_model_idx, 'RMSE']\n",
    "\n",
    "print(f\"\\nBest performing model: {best_model}\")\n",
    "print(f\"Best RMSE: {best_rmse:.4f}\")\n",
    "\n",
    "# Performance improvement analysis\n",
    "baseline_rmse = max(rmse_cnn, rmse_lstm, rmse_hybrid)\n",
    "improvement = (baseline_rmse - best_rmse) / baseline_rmse * 100\n",
    "print(f\"Improvement over worst model: {improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Visual Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visual comparison of model performance\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "models = ['1D CNN', 'LSTM', 'CNN-LSTM']\n",
    "colors = ['blue', 'green', 'purple']\n",
    "\n",
    "# Plot 1: RMSE comparison\n",
    "rmse_values = [rmse_cnn, rmse_lstm, rmse_hybrid]\n",
    "bars1 = ax1.bar(models, rmse_values, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax1.set_title('Root Mean Square Error (RMSE) Comparison', fontsize=14)\n",
    "ax1.set_ylabel('RMSE')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars1, rmse_values):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 2: MAE comparison\n",
    "mae_values = [mae_cnn, mae_lstm, mae_hybrid]\n",
    "bars2 = ax2.bar(models, mae_values, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax2.set_title('Mean Absolute Error (MAE) Comparison', fontsize=14)\n",
    "ax2.set_ylabel('MAE')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "for bar, value in zip(bars2, mae_values):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "             f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 3: Parameters comparison\n",
    "param_values = [total_params, total_params_lstm, total_params_hybrid]\n",
    "bars3 = ax3.bar(models, param_values, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax3.set_title('Model Complexity (Parameters) Comparison', fontsize=14)\n",
    "ax3.set_ylabel('Number of Parameters')\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "for bar, value in zip(bars3, param_values):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 500, \n",
    "             f'{value:,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 4: MAPE comparison\n",
    "mape_values = [mape_cnn, mape_lstm, mape_hybrid]\n",
    "bars4 = ax4.bar(models, mape_values, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax4.set_title('Mean Absolute Percentage Error (MAPE) Comparison', fontsize=14)\n",
    "ax4.set_ylabel('MAPE (%)')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "for bar, value in zip(bars4, mape_values):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "             f'{value:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions from all models\n",
    "# Select subset of test data for clearer visualization\n",
    "viz_length = min(150, len(y_test))\n",
    "viz_start = len(y_test) - viz_length\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Main prediction plot\n",
    "plt.subplot(2, 1, 1)\n",
    "x_axis = range(viz_length)\n",
    "plt.plot(x_axis, y_test[viz_start:], label='True Values', color='black', linewidth=2, alpha=0.8)\n",
    "plt.plot(x_axis, y_pred_cnn[viz_start:], label='CNN Predictions', color='blue', linewidth=1.5, alpha=0.7)\n",
    "plt.plot(x_axis, y_pred_lstm[viz_start:], label='LSTM Predictions', color='green', linewidth=1.5, alpha=0.7)\n",
    "plt.plot(x_axis, y_pred_hybrid[viz_start:], label='CNN-LSTM Predictions', color='purple', linewidth=1.5, alpha=0.7)\n",
    "\n",
    "plt.title(f'Sales Forecasting: Model Predictions Comparison (Last {viz_length} days)', fontsize=16)\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Sales Volume')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Error plot\n",
    "plt.subplot(2, 1, 2)\n",
    "error_cnn = y_test[viz_start:] - y_pred_cnn[viz_start:]\n",
    "error_lstm = y_test[viz_start:] - y_pred_lstm[viz_start:]\n",
    "error_hybrid = y_test[viz_start:] - y_pred_hybrid[viz_start:]\n",
    "\n",
    "plt.plot(x_axis, error_cnn, label='CNN Errors', color='blue', alpha=0.7, linewidth=1)\n",
    "plt.plot(x_axis, error_lstm, label='LSTM Errors', color='green', alpha=0.7, linewidth=1)\n",
    "plt.plot(x_axis, error_hybrid, label='CNN-LSTM Errors', color='purple', alpha=0.7, linewidth=1)\n",
    "plt.axhline(0, color='red', linestyle='--', alpha=0.8)\n",
    "\n",
    "plt.title('Prediction Errors Comparison', fontsize=14)\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Error (True - Predicted)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Error statistics\n",
    "print(f\"\\nError Statistics for Last {viz_length} Predictions:\")\n",
    "print(f\"CNN    - Mean Error: {np.mean(error_cnn):6.3f}, Std Error: {np.std(error_cnn):6.3f}\")\n",
    "print(f\"LSTM   - Mean Error: {np.mean(error_lstm):6.3f}, Std Error: {np.std(error_lstm):6.3f}\")\n",
    "print(f\"Hybrid - Mean Error: {np.mean(error_hybrid):6.3f}, Std Error: {np.std(error_hybrid):6.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Conclusions\n",
    "\n",
    "### 8.1 Key Findings and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DEEP LEARNING TIME SERIES ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nDATASET CHARACTERISTICS:\")\n",
    "print(f\"• Total time series length: {len(sales_data)} days\")\n",
    "print(f\"• Training samples: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"• Testing samples: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "print(f\"• Window size: {window_size} days\")\n",
    "print(f\"• Forecast horizon: {forecast_lag} day\")\n",
    "\n",
    "print(\"\\nMODEL ARCHITECTURES COMPARED:\")\n",
    "print(\"1. 1D CNN: Conv1D → MaxPooling → Conv1D → MaxPooling → Dense\")\n",
    "print(\"2. LSTM: LSTM(50) → LSTM(32) → Dense layers\")\n",
    "print(\"3. CNN-LSTM Hybrid: TimeDistributed(CNN) → LSTM → Dense\")\n",
    "\n",
    "print(\"\\nPERFORMANCE RANKING (by RMSE):\")\n",
    "performance_sorted = performance_df.sort_values('RMSE')\n",
    "for i, (idx, row) in enumerate(performance_sorted.iterrows()):\n",
    "    print(f\"{i+1}. {row['Model']}: RMSE = {row['RMSE']:.4f}, Parameters = {row['Parameters']:,}\")\n",
    "\n",
    "print(\"\\nKEY INSIGHTS:\")\n",
    "if rmse_lstm < rmse_cnn and rmse_lstm < rmse_hybrid:\n",
    "    print(\"• LSTM achieved the best performance for this time series\")\n",
    "    print(\"• Sequential memory capabilities are crucial for sales forecasting\")\nif abs(rmse_lstm - rmse_hybrid) < 0.1:\n",
    "    print(\"• LSTM and CNN-LSTM performance are very similar\")\n",
    "    print(\"• Hybrid architecture didn't provide significant improvement\")\nif total_params_lstm < total_params:\n",
    "    print(\"• LSTM is more parameter-efficient than pure CNN\")\nif total_params_hybrid > total_params_lstm:\n",
    "    print(\"• Hybrid model is most complex but may not justify the complexity\")\n\n",
    "print(\"\\nRECOMMENDATIONS:\")\n",
    "print(\"• LSTM networks excel at capturing temporal dependencies\")\n",
    "print(\"• 1D CNNs are effective for pattern recognition in time series\")\n",
    "print(\"• Hybrid approaches should be evaluated against simpler alternatives\")\n",
    "print(\"• Consider model complexity vs. performance trade-offs\")\n",
    "print(\"• Data scaling is crucial for neural network performance\")\n",
    "print(\"• Temporal train-test splits preserve realistic evaluation\")\n",
    "\n",
    "print(\"\\nFUTURE IMPROVEMENTS:\")\n",
    "print(\"• Experiment with attention mechanisms\")\n",
    "print(\"• Try ensemble methods combining different architectures\")\n",
    "print(\"• Include external features (holidays, promotions, weather)\")\n",
    "print(\"• Implement early stopping and learning rate scheduling\")\n",
    "print(\"• Explore transformer architectures for time series\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}