{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Imputation using MICE\n",
    "\n",
    "**Date:** 9 October 2025\n",
    "\n",
    "**Topic:** Multiple Imputation by Chained Equations for missing data handling\n",
    "\n",
    "This notebook covers:\n",
    "1. Introduction to MICE Algorithm\n",
    "2. Titanic Dataset Imputation\n",
    "3. Life Expectancy Dataset Analysis\n",
    "4. Planets Dataset Application\n",
    "5. Comparative Analysis and Validation\n",
    "6. Best Practices and Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction to MICE Algorithm\n",
    "\n",
    "### 2.1 MICE Methodology Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MULTIPLE IMPUTATION BY CHAINED EQUATIONS (MICE)\")\n",
    "print(\"=\" * 55)\n",
    "print(\"\\nAlgorithm Steps:\")\n",
    "print(\"1. Replace missing values with initial estimates (mean/mode)\")\n",
    "print(\"2. For each variable with missing data:\")\n",
    "print(\"   a. Set current estimates to missing\")\n",
    "print(\"   b. Regress variable on other variables\")\n",
    "print(\"   c. Predict missing values using regression\")\n",
    "print(\"   d. Update estimates with new predictions\")\n",
    "print(\"3. Repeat step 2 for specified iterations\")\n",
    "print(\"4. Output imputed dataset\")\n",
    "\n",
    "print(\"\\nKey Advantages:\")\n",
    "print(\"- Accounts for uncertainty in imputations\")\n",
    "print(\"- Preserves relationships between variables\")\n",
    "print(\"- Flexible for different variable types\")\n",
    "print(\"- Better than single imputation methods\")\n",
    "\n",
    "print(\"\\nLimitations:\")\n",
    "print(\"- Assumes data is Missing at Random (MAR)\")\n",
    "print(\"- Computationally intensive\")\n",
    "print(\"- Requires careful model specification\")\n",
    "print(\"- May not work well with high missingness rates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Titanic Dataset Analysis\n",
    "\n",
    "### 3.1 Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Titanic dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "print(\"Titanic Dataset Information:\")\n",
    "print(f\"Shape: {titanic.shape}\")\n",
    "print(f\"\\nColumns: {list(titanic.columns)}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(titanic.head())\n",
    "\n",
    "# Analyze missing values\n",
    "print(\"\\nMissing Values Analysis:\")\n",
    "missing_counts = titanic.isnull().sum()\n",
    "missing_percentages = (missing_counts / len(titanic)) * 100\n",
    "\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Column': missing_counts.index,\n",
    "    'Missing_Count': missing_counts.values,\n",
    "    'Missing_Percentage': missing_percentages.values\n",
    "})\n",
    "missing_summary = missing_summary[missing_summary['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "print(missing_summary)\n",
    "\n",
    "# Total missingness\n",
    "total_missing = missing_counts.sum()\n",
    "total_cells = titanic.shape[0] * titanic.shape[1]\n",
    "print(f\"\\nTotal missing values: {total_missing}\")\n",
    "print(f\"Overall missingness rate: {(total_missing/total_cells)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data Preprocessing for MICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns and prepare for imputation\n",
    "# Focus on numeric and easily encodable variables\n",
    "titanic_subset = titanic[['age', 'fare', 'pclass', 'sex', 'survived']].copy()\n",
    "\n",
    "# Encode categorical variables\n",
    "titanic_encoded = pd.get_dummies(titanic_subset, columns=['sex'], drop_first=True)\n",
    "\n",
    "print(\"Preprocessed Titanic Dataset:\")\n",
    "print(f\"Shape: {titanic_encoded.shape}\")\n",
    "print(f\"Columns: {list(titanic_encoded.columns)}\")\n",
    "display(titanic_encoded.head())\n",
    "\n",
    "# Check missing values in subset\n",
    "print(\"\\nMissing values in subset:\")\n",
    "subset_missing = titanic_encoded.isnull().sum()\n",
    "print(subset_missing[subset_missing > 0])\n",
    "\n",
    "# Display some rows with missing age values\n",
    "print(\"\\nSample rows with missing age:\")\n",
    "missing_age_sample = titanic_encoded[titanic_encoded['age'].isnull()].head(10)\n",
    "display(missing_age_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 MICE Imputation on Titanic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MICE imputation\n",
    "mice_imputer = IterativeImputer(\n",
    "    max_iter=10,\n",
    "    random_state=42,\n",
    "    estimator=LinearRegression()\n",
    ")\n",
    "\n",
    "# Fit and transform the data\n",
    "titanic_imputed_array = mice_imputer.fit_transform(titanic_encoded)\n",
    "titanic_imputed = pd.DataFrame(\n",
    "    titanic_imputed_array, \n",
    "    columns=titanic_encoded.columns\n",
    ")\n",
    "\n",
    "print(\"MICE Imputation Results - Titanic:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Verify no missing values remain\n",
    "remaining_missing = titanic_imputed.isnull().sum()\n",
    "print(f\"Remaining missing values: {remaining_missing.sum()}\")\n",
    "\n",
    "# Compare original and imputed data\n",
    "print(\"\\nComparison - Original vs Imputed Data:\")\n",
    "comparison_data = pd.DataFrame({\n",
    "    'Original_Age_Mean': titanic_encoded['age'].mean(),\n",
    "    'Imputed_Age_Mean': titanic_imputed['age'].mean(),\n",
    "    'Original_Age_Std': titanic_encoded['age'].std(),\n",
    "    'Imputed_Age_Std': titanic_imputed['age'].std()\n",
    "}, index=[0])\n",
    "\n",
    "display(comparison_data.round(3))\n",
    "\n",
    "# Display sample of imputed values\n",
    "print(\"\\nSample of imputed age values:\")\n",
    "original_missing_mask = titanic_encoded['age'].isnull()\n",
    "imputed_values = titanic_imputed.loc[original_missing_mask, 'age'].head(10)\n",
    "print(imputed_values.round(2).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Titanic Imputation Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization for Titanic imputation\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Age distribution before imputation\n",
    "ax1.hist(titanic_encoded['age'].dropna(), bins=30, alpha=0.7, color='blue', \n",
    "         density=True, label='Original Data')\n",
    "ax1.set_xlabel('Age')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.set_title('Age Distribution - Before Imputation')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Age distribution after imputation\n",
    "ax2.hist(titanic_encoded['age'].dropna(), bins=30, alpha=0.7, color='blue', \n",
    "         density=True, label='Original Data')\n",
    "ax2.hist(titanic_imputed['age'], bins=30, alpha=0.7, color='red', \n",
    "         density=True, label='Imputed Data')\n",
    "ax2.set_xlabel('Age')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.set_title('Age Distribution - After Imputation')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Box plots comparison\n",
    "age_data = [\n",
    "    titanic_encoded['age'].dropna(),\n",
    "    titanic_imputed['age']\n",
    "]\n",
    "ax3.boxplot(age_data, labels=['Original', 'Imputed'])\n",
    "ax3.set_ylabel('Age')\n",
    "ax3.set_title('Age Distribution Comparison (Box Plot)')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Imputed values only\n",
    "imputed_ages = titanic_imputed.loc[original_missing_mask, 'age']\n",
    "ax4.hist(imputed_ages, bins=20, alpha=0.7, color='green', \n",
    "         edgecolor='black', density=True)\n",
    "ax4.set_xlabel('Age')\n",
    "ax4.set_ylabel('Density')\n",
    "ax4.set_title('Distribution of Imputed Age Values Only')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical comparison\n",
    "print(\"Statistical Comparison:\")\n",
    "print(f\"Original age statistics (excluding missing):\")\n",
    "print(f\"  Mean: {titanic_encoded['age'].mean():.2f}\")\n",
    "print(f\"  Median: {titanic_encoded['age'].median():.2f}\")\n",
    "print(f\"  Std: {titanic_encoded['age'].std():.2f}\")\n",
    "\n",
    "print(f\"\\nImputed dataset age statistics:\")\n",
    "print(f\"  Mean: {titanic_imputed['age'].mean():.2f}\")\n",
    "print(f\"  Median: {titanic_imputed['age'].median():.2f}\")\n",
    "print(f\"  Std: {titanic_imputed['age'].std():.2f}\")\n",
    "\n",
    "print(f\"\\nImputed values only statistics:\")\n",
    "print(f\"  Mean: {imputed_ages.mean():.2f}\")\n",
    "print(f\"  Median: {imputed_ages.median():.2f}\")\n",
    "print(f\"  Std: {imputed_ages.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Life Expectancy Dataset Analysis\n",
    "\n",
    "### 4.1 Dataset Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic life expectancy dataset for demonstration\n",
    "# In real scenarios, you would load from the actual WHO dataset\n",
    "np.random.seed(42)\n",
    "n_countries = 500\n",
    "n_years = 5\n",
    "total_rows = n_countries * n_years\n",
    "\n",
    "# Generate synthetic life expectancy data\n",
    "life_expectancy_data = {\n",
    "    'Country': np.repeat([f'Country_{i}' for i in range(n_countries)], n_years),\n",
    "    'Year': np.tile(range(2015, 2020), n_countries),\n",
    "    'Life_expectancy': np.random.normal(70, 10, total_rows),\n",
    "    'Adult_Mortality': np.random.exponential(150, total_rows),\n",
    "    'Alcohol': np.random.exponential(5, total_rows),\n",
    "    'Hepatitis_B': np.random.normal(85, 15, total_rows),\n",
    "    'GDP': np.random.exponential(10000, total_rows),\n",
    "    'Schooling': np.random.normal(12, 3, total_rows)\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "life_expectancy_df = pd.DataFrame(life_expectancy_data)\n",
    "\n",
    "# Introduce missing values randomly\n",
    "missing_rate = 0.15\n",
    "numeric_cols = ['Life_expectancy', 'Adult_Mortality', 'Alcohol', 'Hepatitis_B', 'GDP', 'Schooling']\n",
    "\n",
    "for col in numeric_cols[1:]:  # Don't make target variable missing\n",
    "    n_missing = int(len(life_expectancy_df) * missing_rate * np.random.uniform(0.5, 1.5))\n",
    "    missing_indices = np.random.choice(len(life_expectancy_df), n_missing, replace=False)\n",
    "    life_expectancy_df.loc[missing_indices, col] = np.nan\n",
    "\n",
    "print(\"Synthetic Life Expectancy Dataset:\")\n",
    "print(f\"Shape: {life_expectancy_df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "display(life_expectancy_df.head(10))\n",
    "\n",
    "# Analyze missing patterns\n",
    "print(\"\\nMissing Values Analysis:\")\n",
    "missing_analysis = life_expectancy_df.isnull().sum()\n",
    "missing_percentages = (missing_analysis / len(life_expectancy_df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_analysis.index,\n",
    "    'Missing_Count': missing_analysis.values,\n",
    "    'Missing_Percentage': missing_percentages.values\n",
    "})\n",
    "display(missing_df[missing_df['Missing_Count'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Life Expectancy MICE Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for imputation (numeric columns only)\n",
    "life_expectancy_numeric = life_expectancy_df[numeric_cols].copy()\n",
    "\n",
    "print(\"Pre-imputation summary:\")\n",
    "print(life_expectancy_numeric.describe())\n",
    "\n",
    "# Apply MICE imputation with Random Forest estimator for better performance\n",
    "mice_imputer_life = IterativeImputer(\n",
    "    max_iter=20,\n",
    "    random_state=42,\n",
    "    estimator=RandomForestRegressor(n_estimators=10, random_state=42)\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "life_expectancy_imputed_array = mice_imputer_life.fit_transform(life_expectancy_numeric)\n",
    "life_expectancy_imputed = pd.DataFrame(\n",
    "    life_expectancy_imputed_array,\n",
    "    columns=numeric_cols\n",
    ")\n",
    "\n",
    "# Add back non-numeric columns\n",
    "life_expectancy_imputed['Country'] = life_expectancy_df['Country']\n",
    "life_expectancy_imputed['Year'] = life_expectancy_df['Year']\n",
    "\n",
    "print(\"\\nPost-imputation verification:\")\n",
    "print(f\"Remaining missing values: {life_expectancy_imputed.isnull().sum().sum()}\")\n",
    "\n",
    "print(\"\\nPost-imputation summary:\")\n",
    "print(life_expectancy_imputed[numeric_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Life Expectancy Imputation Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distributions before and after imputation\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Original data (non-missing)\n",
    "    original_data = life_expectancy_numeric[col].dropna()\n",
    "    ax.hist(original_data, bins=30, alpha=0.6, color='blue', \n",
    "            density=True, label='Original')\n",
    "    \n",
    "    # Imputed complete dataset\n",
    "    ax.hist(life_expectancy_imputed[col], bins=30, alpha=0.6, color='red', \n",
    "            density=True, label='Imputed')\n",
    "    \n",
    "    ax.set_title(f'{col} Distribution Comparison')\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical comparison table\n",
    "comparison_stats = []\n",
    "for col in numeric_cols:\n",
    "    original = life_expectancy_numeric[col].dropna()\n",
    "    imputed = life_expectancy_imputed[col]\n",
    "    \n",
    "    comparison_stats.append({\n",
    "        'Variable': col,\n",
    "        'Original_Mean': original.mean(),\n",
    "        'Imputed_Mean': imputed.mean(),\n",
    "        'Original_Std': original.std(),\n",
    "        'Imputed_Std': imputed.std(),\n",
    "        'Mean_Diff': abs(original.mean() - imputed.mean()),\n",
    "        'Std_Diff': abs(original.std() - imputed.std())\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_stats)\n",
    "print(\"\\nStatistical Comparison - Life Expectancy Dataset:\")\n",
    "display(comparison_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Planets Dataset Application\n",
    "\n",
    "### 5.1 Planets Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load planets dataset\n",
    "planets_df = sns.load_dataset(\"planets\")\n",
    "\n",
    "print(\"Planets Dataset Information:\")\n",
    "print(f\"Shape: {planets_df.shape}\")\n",
    "print(f\"\\nColumns: {list(planets_df.columns)}\")\n",
    "display(planets_df.head())\n",
    "\n",
    "# Check missing values\n",
    "print(\"\\nMissing Values in Planets Dataset:\")\n",
    "planets_missing = planets_df.isnull().sum()\n",
    "planets_missing_df = pd.DataFrame({\n",
    "    'Column': planets_missing.index,\n",
    "    'Missing_Count': planets_missing.values,\n",
    "    'Missing_Percentage': (planets_missing.values / len(planets_df)) * 100\n",
    "})\n",
    "display(planets_missing_df[planets_missing_df['Missing_Count'] > 0])\n",
    "\n",
    "# Focus on numeric variables with missing values\n",
    "numeric_planets = planets_df[['orbital_period', 'mass', 'distance']].copy()\n",
    "print(\"\\nNumeric subset for imputation:\")\n",
    "print(f\"Shape: {numeric_planets.shape}\")\n",
    "display(numeric_planets.head(10))\n",
    "\n",
    "# Show some missing patterns\n",
    "print(\"\\nRows with missing values (sample):\")\n",
    "missing_rows = numeric_planets[numeric_planets.isnull().any(axis=1)]\n",
    "display(missing_rows.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Planets MICE Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MICE to planets dataset\n",
    "mice_imputer_planets = IterativeImputer(\n",
    "    max_iter=15,\n",
    "    random_state=42,\n",
    "    estimator=RandomForestRegressor(n_estimators=20, random_state=42)\n",
    ")\n",
    "\n",
    "# Transform the data\n",
    "planets_imputed_array = mice_imputer_planets.fit_transform(numeric_planets)\n",
    "planets_imputed = pd.DataFrame(\n",
    "    planets_imputed_array,\n",
    "    columns=numeric_planets.columns\n",
    ")\n",
    "\n",
    "print(\"Planets MICE Imputation Results:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Verification\n",
    "print(f\"Original missing values: {numeric_planets.isnull().sum().sum()}\")\n",
    "print(f\"Remaining missing values: {planets_imputed.isnull().sum().sum()}\")\n",
    "\n",
    "# Compare before and after statistics\n",
    "print(\"\\nBefore Imputation:\")\n",
    "display(numeric_planets.describe())\n",
    "\n",
    "print(\"\\nAfter Imputation:\")\n",
    "display(planets_imputed.describe())\n",
    "\n",
    "# Show examples of imputed values\n",
    "print(\"\\nExample imputed values:\")\n",
    "mass_missing_mask = numeric_planets['mass'].isnull()\n",
    "if mass_missing_mask.sum() > 0:\n",
    "    print(f\"Sample imputed mass values: {planets_imputed.loc[mass_missing_mask, 'mass'].head().round(3).tolist()}\")\n",
    "\n",
    "distance_missing_mask = numeric_planets['distance'].isnull()\n",
    "if distance_missing_mask.sum() > 0:\n",
    "    print(f\"Sample imputed distance values: {planets_imputed.loc[distance_missing_mask, 'distance'].head().round(2).tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Planets Imputation Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization for planets imputation\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "variables = ['orbital_period', 'mass', 'distance']\n",
    "colors = ['blue', 'green', 'orange']\n",
    "\n",
    "for i, (var, color) in enumerate(zip(variables, colors)):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Original data (non-missing only)\n",
    "    original_data = numeric_planets[var].dropna()\n",
    "    \n",
    "    # Use log scale for better visualization due to wide ranges\n",
    "    if len(original_data) > 0:\n",
    "        ax.hist(np.log10(original_data + 1), bins=25, alpha=0.6, \n",
    "                color='blue', density=True, label='Original')\n",
    "    \n",
    "    ax.hist(np.log10(planets_imputed[var] + 1), bins=25, alpha=0.6, \n",
    "            color=color, density=True, label='Imputed')\n",
    "    \n",
    "    ax.set_title(f'{var} Distribution (Log Scale)')\n",
    "    ax.set_xlabel(f'log10({var} + 1)')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation analysis\n",
    "print(\"\\nCorrelation Analysis:\")\n",
    "print(\"Original data correlations:\")\n",
    "original_corr = numeric_planets.corr()\n",
    "print(original_corr.round(3))\n",
    "\n",
    "print(\"\\nImputed data correlations:\")\n",
    "imputed_corr = planets_imputed.corr()\n",
    "print(imputed_corr.round(3))\n",
    "\n",
    "print(\"\\nCorrelation preservation:\")\n",
    "corr_diff = abs(original_corr - imputed_corr)\n",
    "print(f\"Maximum correlation difference: {corr_diff.max().max():.4f}\")\n",
    "print(f\"Average correlation difference: {corr_diff.mean().mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparative Analysis and Validation\n",
    "\n",
    "### 6.1 Comparison with Simple Imputation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare MICE with simple imputation methods using Titanic dataset\n",
    "print(\"IMPUTATION METHODS COMPARISON\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Prepare test data (Titanic age column)\n",
    "test_data = titanic_encoded[['age', 'fare', 'pclass']].copy()\n",
    "original_age = test_data['age'].copy()\n",
    "\n",
    "# Method 1: Mean imputation\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "age_mean_imputed = mean_imputer.fit_transform(test_data[['age']])[:, 0]\n",
    "\n",
    "# Method 2: Median imputation\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "age_median_imputed = median_imputer.fit_transform(test_data[['age']])[:, 0]\n",
    "\n",
    "# Method 3: MICE imputation\n",
    "mice_imputer_comp = IterativeImputer(max_iter=10, random_state=42)\n",
    "age_mice_imputed = mice_imputer_comp.fit_transform(test_data)[:, 0]\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_results = pd.DataFrame({\n",
    "    'Method': ['Original', 'Mean Imputation', 'Median Imputation', 'MICE'],\n",
    "    'Mean': [\n",
    "        original_age.mean(),\n",
    "        np.mean(age_mean_imputed),\n",
    "        np.mean(age_median_imputed),\n",
    "        np.mean(age_mice_imputed)\n",
    "    ],\n",
    "    'Std': [\n",
    "        original_age.std(),\n",
    "        np.std(age_mean_imputed),\n",
    "        np.std(age_median_imputed),\n",
    "        np.std(age_mice_imputed)\n",
    "    ],\n",
    "    'Min': [\n",
    "        original_age.min(),\n",
    "        np.min(age_mean_imputed),\n",
    "        np.min(age_median_imputed),\n",
    "        np.min(age_mice_imputed)\n",
    "    ],\n",
    "    'Max': [\n",
    "        original_age.max(),\n",
    "        np.max(age_mean_imputed),\n",
    "        np.max(age_median_imputed),\n",
    "        np.max(age_mice_imputed)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nStatistical Comparison of Imputation Methods:\")\n",
    "display(comparison_results.round(3))\n",
    "\n",
    "# Visualize comparison\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Create subplots for each method\n",
    "methods_data = {\n",
    "    'Original (no missing)': original_age.dropna(),\n",
    "    'Mean Imputation': age_mean_imputed,\n",
    "    'Median Imputation': age_median_imputed,\n",
    "    'MICE': age_mice_imputed\n",
    "}\n",
    "\n",
    "colors = ['blue', 'red', 'green', 'purple']\n",
    "\n",
    "for i, (method, data) in enumerate(methods_data.items()):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.hist(data, bins=25, alpha=0.7, color=colors[i], density=True)\n",
    "    plt.title(f'{method}\\nMean: {np.mean(data):.1f}, Std: {np.std(data):.1f}')\n",
    "    plt.xlabel('Age')\n",
    "    plt.ylabel('Density')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Validation and Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform validation using artificial missingness\n",
    "print(\"MICE VALIDATION ANALYSIS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Create complete dataset for validation\n",
    "validation_data = planets_df[['orbital_period', 'mass', 'distance']].dropna().copy()\n",
    "print(f\"Complete cases for validation: {len(validation_data)}\")\n",
    "\n",
    "if len(validation_data) > 50:  # Ensure we have enough data for validation\n",
    "    # Introduce artificial missingness\n",
    "    validation_data_missing = validation_data.copy()\n",
    "    n_samples = len(validation_data_missing)\n",
    "    \n",
    "    # Randomly remove 20% of mass values\n",
    "    np.random.seed(42)\n",
    "    missing_indices = np.random.choice(n_samples, int(n_samples * 0.2), replace=False)\n",
    "    true_values = validation_data_missing.loc[missing_indices, 'mass'].copy()\n",
    "    validation_data_missing.loc[missing_indices, 'mass'] = np.nan\n",
    "    \n",
    "    # Apply MICE imputation\n",
    "    mice_validator = IterativeImputer(max_iter=10, random_state=42)\n",
    "    validation_imputed = mice_validator.fit_transform(validation_data_missing)\n",
    "    imputed_values = validation_imputed[missing_indices, 1]  # mass is column 1\n",
    "    \n",
    "    # Calculate validation metrics\n",
    "    mae = mean_absolute_error(true_values, imputed_values)\n",
    "    mse = mean_squared_error(true_values, imputed_values)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # Calculate correlation\n",
    "    correlation = np.corrcoef(true_values, imputed_values)[0, 1]\n",
    "    \n",
    "    print(\"\\nValidation Results (Artificial Missingness):\")\n",
    "    print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "    print(f\"Root Mean Square Error: {rmse:.4f}\")\n",
    "    print(f\"Correlation (true vs imputed): {correlation:.4f}\")\n",
    "    \n",
    "    # Visualization of validation\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Scatter plot of true vs imputed\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(true_values, imputed_values, alpha=0.6)\n",
    "    plt.plot([true_values.min(), true_values.max()], \n",
    "             [true_values.min(), true_values.max()], 'r--', lw=2)\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Imputed Values')\n",
    "    plt.title(f'True vs Imputed Values\\nCorrelation: {correlation:.3f}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residuals plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    residuals = true_values - imputed_values\n",
    "    plt.scatter(true_values, residuals, alpha=0.6)\n",
    "    plt.axhline(0, color='r', linestyle='--', lw=2)\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Residuals (True - Imputed)')\n",
    "    plt.title(f'Imputation Residuals\\nRMSE: {rmse:.3f}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"Insufficient complete cases for validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Best Practices and Recommendations\n",
    "\n",
    "### 7.1 Summary and Guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MICE IMPLEMENTATION SUMMARY\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(\"\\nDATASETS PROCESSED:\")\n",
    "datasets_summary = [\n",
    "    ('Titanic', titanic_encoded.shape[0], 'age', titanic_encoded['age'].isnull().sum()),\n",
    "    ('Life Expectancy', life_expectancy_df.shape[0], 'multiple', life_expectancy_numeric.isnull().sum().sum()),\n",
    "    ('Planets', planets_df.shape[0], 'mass/distance', numeric_planets.isnull().sum().sum())\n",
    "]\n",
    "\n",
    "for name, n_rows, missing_vars, n_missing in datasets_summary:\n",
    "    missing_rate = (n_missing / (n_rows * len(missing_vars.split('/')))) * 100 if '/' not in missing_vars else (n_missing / (n_rows * 3)) * 100\n",
    "    print(f\"{name:15}: {n_rows:4} rows, {missing_vars:15}, {n_missing:3} missing ({missing_rate:5.1f}%)\")\n",
    "\n",
    "print(\"\\nKEY FINDINGS:\")\n",
    "print(\"1. MICE preserves distributional properties better than simple imputation\")\n",
    "print(\"2. Random Forest estimator often performs better than Linear Regression\")\n",
    "print(\"3. Iterative process allows for complex variable relationships\")\n",
    "print(\"4. Correlation structure is generally well-preserved\")\n",
    "print(\"5. Performance depends on missingness mechanism and rate\")\n",
    "\n",
    "print(\"\\nBEST PRACTICES:\")\n",
    "print(\"• Analyze missingness patterns before imputation\")\n",
    "print(\"• Choose appropriate estimator (Linear vs Tree-based)\")\n",
    "print(\"• Set sufficient iterations (typically 10-20)\")\n",
    "print(\"• Validate with artificial missingness when possible\")\n",
    "print(\"• Consider multiple imputation for uncertainty quantification\")\n",
    "print(\"• Monitor convergence of imputed values\")\n",
    "\n",
    "print(\"\\nLIMITATIONS TO CONSIDER:\")\n",
    "print(\"• Assumes Missing at Random (MAR) mechanism\")\n",
    "print(\"• Computationally intensive for large datasets\")\n",
    "print(\"• May not work well with high missingness rates (>50%)\")\n",
    "print(\"• Requires careful handling of categorical variables\")\n",
    "print(\"• Can introduce artificial precision\")\n",
    "\n",
    "print(\"\\nALTERNATIVE APPROACHES:\")\n",
    "print(\"• K-Nearest Neighbors (KNN) imputation\")\n",
    "print(\"• Matrix factorization methods\")\n",
    "print(\"• Deep learning-based imputation\")\n",
    "print(\"• Domain-specific imputation strategies\")\n",
    "print(\"• Multiple imputation with proper pooling\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}