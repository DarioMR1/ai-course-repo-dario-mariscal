{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Time Series Analysis\n",
    "\n",
    "**Date:** 30 October 2025\n",
    "\n",
    "**Topic:** Fundamental concepts and techniques in time series analysis\n",
    "\n",
    "This notebook covers:\n",
    "1. Time Series Data Loading and Visualization\n",
    "2. Stationarity Testing (ADF and KPSS Tests)\n",
    "3. Autocorrelation and Partial Autocorrelation Analysis\n",
    "4. Time Series Components and Decomposition\n",
    "5. Basic Forecasting Methods\n",
    "6. Model Evaluation Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Time series specific libraries\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Statistical libraries\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Synthetic Energy Consumption Dataset\n",
    "\n",
    "### 2.1 Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic energy consumption dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate time series data for one year with hourly frequency\n",
    "start_date = '2022-01-01'\n",
    "end_date = '2022-12-31 23:00:00'\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='H')\n",
    "\n",
    "n_observations = len(date_range)\n",
    "print(f\"Generating {n_observations} hourly observations from {start_date} to {end_date}\")\n",
    "\n",
    "# Generate synthetic features\n",
    "np.random.seed(42)\n",
    "temperature_base = 20 + 10 * np.sin(2 * np.pi * np.arange(n_observations) / (24 * 365)) + np.random.normal(0, 2, n_observations)\n",
    "humidity = np.random.uniform(30, 80, n_observations)\n",
    "square_footage = np.random.uniform(1000, 2000, n_observations)\n",
    "occupancy = np.random.poisson(5, n_observations)\n",
    "renewable_energy = np.random.exponential(10, n_observations)\n",
    "\n",
    "# Create realistic energy consumption pattern\n",
    "# Base consumption with daily and seasonal patterns\n",
    "daily_pattern = 50 + 20 * np.sin(2 * np.pi * np.arange(n_observations) / 24)  # Daily cycle\n",
    "seasonal_pattern = 10 * np.sin(2 * np.pi * np.arange(n_observations) / (24 * 365))  # Seasonal cycle\n",
    "temperature_effect = 0.5 * np.abs(temperature_base - 22)  # Higher consumption when temp far from 22°C\n",
    "occupancy_effect = 2 * occupancy\n",
    "random_noise = np.random.normal(0, 5, n_observations)\n",
    "\n",
    "energy_consumption = (daily_pattern + seasonal_pattern + temperature_effect + \n",
    "                     occupancy_effect + random_noise)\n",
    "\n",
    "# Ensure positive values\n",
    "energy_consumption = np.maximum(energy_consumption, 10)\n",
    "\n",
    "# Create DataFrame\n",
    "energy_data = pd.DataFrame({\n",
    "    'Timestamp': date_range,\n",
    "    'Temperature': temperature_base,\n",
    "    'Humidity': humidity,\n",
    "    'SquareFootage': square_footage,\n",
    "    'Occupancy': occupancy,\n",
    "    'HVACUsage': np.random.choice(['On', 'Off'], n_observations),\n",
    "    'LightingUsage': np.random.choice(['On', 'Off'], n_observations),\n",
    "    'RenewableEnergy': renewable_energy,\n",
    "    'DayOfWeek': [date.strftime('%A') for date in date_range],\n",
    "    'Holiday': np.random.choice(['Yes', 'No'], n_observations, p=[0.05, 0.95]),\n",
    "    'EnergyConsumption': energy_consumption\n",
    "})\n",
    "\n",
    "# Set timestamp as index\n",
    "energy_data = energy_data.set_index('Timestamp')\n",
    "\n",
    "print(f\"\\nDataset created with shape: {energy_data.shape}\")\n",
    "print(f\"Date range: {energy_data.index.min()} to {energy_data.index.max()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "display(energy_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values and basic statistics\n",
    "print(\"Data Quality Assessment:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Missing values\n",
    "missing_values = energy_data.isnull().sum()\n",
    "print(f\"\\nMissing values per column:\")\n",
    "for col, missing in missing_values.items():\n",
    "    print(f\"{col:20}: {missing:3}\")\n",
    "\n",
    "print(f\"\\nTotal missing values: {missing_values.sum()}\")\n",
    "\n",
    "# Basic statistics for numeric columns\n",
    "print(f\"\\nBasic statistics for key variables:\")\n",
    "key_vars = ['Temperature', 'EnergyConsumption', 'Occupancy', 'RenewableEnergy']\n",
    "display(energy_data[key_vars].describe())\n",
    "\n",
    "# Check data types\n",
    "print(f\"\\nData types:\")\n",
    "for col, dtype in energy_data.dtypes.items():\n",
    "    print(f\"{col:20}: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Time Series Visualization\n",
    "\n",
    "### 3.1 Interactive Time Series Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive plot using Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=energy_data.index,\n",
    "        y=energy_data['EnergyConsumption'],\n",
    "        mode='lines',\n",
    "        name='Energy Consumption',\n",
    "        line=dict(color='blue', width=1)\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Energy Consumption Over Time (Hourly Data)\",\n",
    "        'x': 0.5,\n",
    "        'xanchor': 'center'\n",
    "    },\n",
    "    xaxis_title=\"Time\",\n",
    "    yaxis_title=\"Energy Consumption (kWh)\",\n",
    "    template=\"plotly_white\",\n",
    "    width=1000,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nEnergy Consumption Summary:\")\n",
    "print(f\"Mean: {energy_data['EnergyConsumption'].mean():.2f} kWh\")\n",
    "print(f\"Std: {energy_data['EnergyConsumption'].std():.2f} kWh\")\n",
    "print(f\"Min: {energy_data['EnergyConsumption'].min():.2f} kWh\")\n",
    "print(f\"Max: {energy_data['EnergyConsumption'].max():.2f} kWh\")\n",
    "print(f\"Range: {energy_data['EnergyConsumption'].max() - energy_data['EnergyConsumption'].min():.2f} kWh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Multiple Time Series Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots for multiple variables\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Energy Consumption', 'Temperature', 'Occupancy', 'Renewable Energy'),\n",
    "    vertical_spacing=0.08\n",
    ")\n",
    "\n",
    "# Sample data for better visualization (every 24th point for daily averages)\n",
    "sample_data = energy_data.resample('D').mean()\n",
    "\n",
    "# Energy Consumption\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=sample_data.index, y=sample_data['EnergyConsumption'],\n",
    "              mode='lines', name='Energy Consumption', line=dict(color='blue')),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Temperature\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=sample_data.index, y=sample_data['Temperature'],\n",
    "              mode='lines', name='Temperature', line=dict(color='red')),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Occupancy\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=sample_data.index, y=sample_data['Occupancy'],\n",
    "              mode='lines', name='Occupancy', line=dict(color='green')),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Renewable Energy\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=sample_data.index, y=sample_data['RenewableEnergy'],\n",
    "              mode='lines', name='Renewable Energy', line=dict(color='orange')),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Daily Average Time Series Components\",\n",
    "    showlegend=False,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stationarity Testing\n",
    "\n",
    "### 4.1 Statistical Tests Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stationarity testing functions\n",
    "def adf_test(series, name=\"Series\"):\n",
    "    \"\"\"\n",
    "    Perform Augmented Dickey-Fuller test for stationarity\n",
    "    H0: Series has unit root (non-stationary)\n",
    "    H1: Series is stationary\n",
    "    \"\"\"\n",
    "    result = adfuller(series, autolag=\"AIC\")\n",
    "    \n",
    "    output = {\n",
    "        'test_statistic': result[0],\n",
    "        'pvalue': result[1],\n",
    "        'n_lags': result[2],\n",
    "        'n_observations': result[3],\n",
    "        'critical_values': result[4],\n",
    "        'ic_best': result[5]\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nAugmented Dickey-Fuller Test - {name}\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Test Statistic: {output['test_statistic']:.6f}\")\n",
    "    print(f\"p-value: {output['pvalue']:.6f}\")\n",
    "    print(f\"Number of lags: {output['n_lags']}\")\n",
    "    print(f\"Number of observations: {output['n_observations']}\")\n",
    "    \n",
    "    print(\"\\nCritical Values:\")\n",
    "    for key, value in output['critical_values'].items():\n",
    "        print(f\"\\t{key}: {value:.3f}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    alpha = 0.05\n",
    "    if output['pvalue'] <= alpha:\n",
    "        print(f\"\\nResult: STATIONARY (p-value {output['pvalue']:.6f} <= {alpha})\")\n",
    "        print(\"Reject null hypothesis - series does not have unit root\")\n",
    "    else:\n",
    "        print(f\"\\nResult: NON-STATIONARY (p-value {output['pvalue']:.6f} > {alpha})\")\n",
    "        print(\"Fail to reject null hypothesis - series has unit root\")\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def kpss_test(series, regression=\"c\", name=\"Series\"):\n",
    "    \"\"\"\n",
    "    Perform KPSS test for stationarity\n",
    "    H0: Series is stationary\n",
    "    H1: Series has unit root (non-stationary)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        statistic, pvalue, n_lags, critical_values = kpss(series, regression=regression, nlags=\"auto\")\n",
    "        \n",
    "        output = {\n",
    "            'test_statistic': statistic,\n",
    "            'pvalue': pvalue,\n",
    "            'n_lags': n_lags,\n",
    "            'critical_values': critical_values,\n",
    "            'regression': regression\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nKPSS Test - {name}\")\n",
    "        print(\"=\" * 30)\n",
    "        print(f\"Test Statistic: {output['test_statistic']:.6f}\")\n",
    "        print(f\"p-value: {output['pvalue']:.6f}\")\n",
    "        print(f\"Number of lags: {output['n_lags']}\")\n",
    "        print(f\"Regression type: {output['regression']}\")\n",
    "        \n",
    "        print(\"\\nCritical Values:\")\n",
    "        for key, value in output['critical_values'].items():\n",
    "            print(f\"\\t{key}: {value:.3f}\")\n",
    "        \n",
    "        # Interpretation\n",
    "        alpha = 0.05\n",
    "        if output['pvalue'] <= alpha:\n",
    "            print(f\"\\nResult: NON-STATIONARY (p-value {output['pvalue']:.6f} <= {alpha})\")\n",
    "            print(\"Reject null hypothesis - series is not stationary\")\n",
    "        else:\n",
    "            print(f\"\\nResult: STATIONARY (p-value {output['pvalue']:.6f} > {alpha})\")\n",
    "            print(\"Fail to reject null hypothesis - series is stationary\")\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nKPSS test failed for {name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def comprehensive_stationarity_test(series, name=\"Series\"):\n",
    "    \"\"\"\n",
    "    Perform both ADF and KPSS tests and provide combined interpretation\n",
    "    \"\"\"\n",
    "    print(f\"\\nCOMPREHENSIVE STATIONARITY ANALYSIS: {name.upper()}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Perform tests\n",
    "    adf_result = adf_test(series, name)\n",
    "    kpss_result = kpss_test(series, name=name)\n",
    "    \n",
    "    # Combined interpretation\n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(\"COMBINED INTERPRETATION:\")\n",
    "    \n",
    "    adf_stationary = adf_result['pvalue'] <= 0.05\n",
    "    kpss_stationary = (kpss_result['pvalue'] > 0.05) if kpss_result else None\n",
    "    \n",
    "    if adf_stationary and kpss_stationary:\n",
    "        conclusion = \"STATIONARY - Both tests agree\"\n",
    "    elif not adf_stationary and not kpss_stationary:\n",
    "        conclusion = \"NON-STATIONARY - Both tests agree\"\n",
    "    elif adf_stationary and not kpss_stationary:\n",
    "        conclusion = \"DIFFERENCE STATIONARY - Mixed signals\"\n",
    "    elif not adf_stationary and kpss_stationary:\n",
    "        conclusion = \"TREND STATIONARY - Mixed signals\"\n",
    "    else:\n",
    "        conclusion = \"INCONCLUSIVE - Unable to determine\"\n",
    "    \n",
    "    print(f\"Final Conclusion: {conclusion}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return {\n",
    "        'adf_result': adf_result,\n",
    "        'kpss_result': kpss_result,\n",
    "        'conclusion': conclusion\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Energy Consumption Stationarity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test stationarity of energy consumption series\n",
    "energy_series = energy_data['EnergyConsumption']\n",
    "stationarity_result = comprehensive_stationarity_test(energy_series, \"Energy Consumption\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Differencing for Stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If series is non-stationary, apply differencing\n",
    "if \"NON-STATIONARY\" in stationarity_result['conclusion']:\n",
    "    print(\"\\nApplying first differencing to achieve stationarity...\")\n",
    "    \n",
    "    # First difference\n",
    "    energy_diff = energy_series.diff().dropna()\n",
    "    \n",
    "    # Test stationarity of differenced series\n",
    "    diff_result = comprehensive_stationarity_test(energy_diff, \"First Differenced Energy Consumption\")\n",
    "    \n",
    "    # Visualize original vs differenced series\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "    \n",
    "    # Original series\n",
    "    ax1.plot(energy_series.index[:1000], energy_series.iloc[:1000], color='blue', linewidth=1)\n",
    "    ax1.set_title('Original Energy Consumption Series (First 1000 observations)')\n",
    "    ax1.set_ylabel('Energy Consumption (kWh)')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Differenced series\n",
    "    ax2.plot(energy_diff.index[:1000], energy_diff.iloc[:1000], color='red', linewidth=1)\n",
    "    ax2.set_title('First Differenced Energy Consumption Series (First 1000 observations)')\n",
    "    ax2.set_ylabel('Change in Energy Consumption')\n",
    "    ax2.set_xlabel('Time')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"\\nSeries is already stationary - no differencing needed.\")\n",
    "    energy_diff = energy_series\n",
    "    diff_result = stationarity_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Autocorrelation Analysis\n",
    "\n",
    "### 5.1 ACF and PACF Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze autocorrelation patterns\n",
    "print(\"AUTOCORRELATION ANALYSIS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Use a subset of data for clearer visualization\n",
    "analysis_series = energy_series.iloc[:2000]  # First 2000 observations\n",
    "\n",
    "# Create ACF and PACF plots\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Original series plot\n",
    "ax1.plot(analysis_series.index, analysis_series.values, color='blue', linewidth=1)\n",
    "ax1.set_title('Energy Consumption Time Series')\n",
    "ax1.set_ylabel('Energy Consumption (kWh)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# ACF plot\n",
    "plot_acf(analysis_series, ax=ax2, lags=50, alpha=0.05, title=\"Autocorrelation Function (ACF)\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# PACF plot\n",
    "plot_pacf(analysis_series, ax=ax3, lags=50, alpha=0.05, title=\"Partial Autocorrelation Function (PACF)\")\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution of the series\n",
    "ax4.hist(analysis_series, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "ax4.set_title('Distribution of Energy Consumption')\n",
    "ax4.set_xlabel('Energy Consumption (kWh)')\n",
    "ax4.set_ylabel('Frequency')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpretation of ACF and PACF\n",
    "print(\"\\nACF and PACF Interpretation:\")\n",
    "print(\"- ACF shows correlation between observations and lagged versions\")\n",
    "print(\"- PACF shows direct correlation after removing effects of shorter lags\")\n",
    "print(\"- Significant spikes suggest potential AR/MA components for modeling\")\n",
    "print(\"- Seasonal patterns may be visible at regular intervals (e.g., 24 hours, 168 hours)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Seasonal Patterns Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze daily and weekly patterns\n",
    "print(\"SEASONAL PATTERN ANALYSIS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Create time-based features\n",
    "energy_analysis = energy_data.copy()\n",
    "energy_analysis['Hour'] = energy_analysis.index.hour\n",
    "energy_analysis['DayOfWeek'] = energy_analysis.index.dayofweek\n",
    "energy_analysis['Month'] = energy_analysis.index.month\n",
    "\n",
    "# Daily pattern analysis\n",
    "hourly_avg = energy_analysis.groupby('Hour')['EnergyConsumption'].mean()\n",
    "weekly_avg = energy_analysis.groupby('DayOfWeek')['EnergyConsumption'].mean()\n",
    "monthly_avg = energy_analysis.groupby('Month')['EnergyConsumption'].mean()\n",
    "\n",
    "# Visualize patterns\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Daily pattern\n",
    "ax1.plot(hourly_avg.index, hourly_avg.values, marker='o', linewidth=2, markersize=6)\n",
    "ax1.set_title('Average Energy Consumption by Hour')\n",
    "ax1.set_xlabel('Hour of Day')\n",
    "ax1.set_ylabel('Avg Energy Consumption (kWh)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xticks(range(0, 24, 4))\n",
    "\n",
    "# Weekly pattern\n",
    "days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "ax2.bar(range(7), weekly_avg.values, color='lightblue', edgecolor='black')\n",
    "ax2.set_title('Average Energy Consumption by Day of Week')\n",
    "ax2.set_xlabel('Day of Week')\n",
    "ax2.set_ylabel('Avg Energy Consumption (kWh)')\n",
    "ax2.set_xticks(range(7))\n",
    "ax2.set_xticklabels(days)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Monthly pattern\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "ax3.plot(range(1, 13), monthly_avg.values, marker='s', linewidth=2, markersize=6, color='green')\n",
    "ax3.set_title('Average Energy Consumption by Month')\n",
    "ax3.set_xlabel('Month')\n",
    "ax3.set_ylabel('Avg Energy Consumption (kWh)')\n",
    "ax3.set_xticks(range(1, 13))\n",
    "ax3.set_xticklabels(months, rotation=45)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print pattern insights\n",
    "print(f\"\\nSeasonal Pattern Insights:\")\n",
    "print(f\"Peak daily hour: {hourly_avg.idxmax()}:00 ({hourly_avg.max():.1f} kWh)\")\n",
    "print(f\"Low daily hour: {hourly_avg.idxmin()}:00 ({hourly_avg.min():.1f} kWh)\")\n",
    "print(f\"Highest consumption day: {days[weekly_avg.idxmax()]} ({weekly_avg.max():.1f} kWh)\")\n",
    "print(f\"Lowest consumption day: {days[weekly_avg.idxmin()]} ({weekly_avg.min():.1f} kWh)\")\n",
    "print(f\"Peak month: {months[monthly_avg.idxmax()-1]} ({monthly_avg.max():.1f} kWh)\")\n",
    "print(f\"Low month: {months[monthly_avg.idxmin()-1]} ({monthly_avg.min():.1f} kWh)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Time Series Decomposition\n",
    "\n",
    "### 6.1 Classical Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform time series decomposition\n",
    "print(\"TIME SERIES DECOMPOSITION\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Use daily data for decomposition (to reduce computational load)\n",
    "daily_energy = energy_data['EnergyConsumption'].resample('D').mean()\n",
    "\n",
    "print(f\"Decomposing daily average energy consumption\")\n",
    "print(f\"Data points: {len(daily_energy)}\")\n",
    "print(f\"Period: {daily_energy.index.min()} to {daily_energy.index.max()}\")\n",
    "\n",
    "# Perform additive decomposition with weekly seasonality\n",
    "decomposition = seasonal_decompose(\n",
    "    daily_energy, \n",
    "    model='additive', \n",
    "    period=7  # Weekly seasonality\n",
    ")\n",
    "\n",
    "# Create decomposition plot\n",
    "fig, axes = plt.subplots(4, 1, figsize=(15, 12))\n",
    "\n",
    "# Original series\n",
    "decomposition.observed.plot(ax=axes[0], title='Original Series', color='blue')\n",
    "axes[0].set_ylabel('Energy (kWh)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Trend component\n",
    "decomposition.trend.plot(ax=axes[1], title='Trend Component', color='red')\n",
    "axes[1].set_ylabel('Trend')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Seasonal component\n",
    "decomposition.seasonal.plot(ax=axes[2], title='Seasonal Component', color='green')\n",
    "axes[2].set_ylabel('Seasonal')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Residual component\n",
    "decomposition.resid.plot(ax=axes[3], title='Residual Component', color='orange')\n",
    "axes[3].set_ylabel('Residual')\n",
    "axes[3].set_xlabel('Date')\n",
    "axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Decomposition analysis\n",
    "print(f\"\\nDecomposition Analysis:\")\n",
    "print(f\"Trend variance: {decomposition.trend.var():.2f}\")\n",
    "print(f\"Seasonal variance: {decomposition.seasonal.var():.2f}\")\n",
    "print(f\"Residual variance: {decomposition.resid.var():.2f}\")\n",
    "\n",
    "# Calculate proportion of variance explained by each component\n",
    "total_var = daily_energy.var()\n",
    "trend_prop = (decomposition.trend.var() / total_var) * 100\n",
    "seasonal_prop = (decomposition.seasonal.var() / total_var) * 100\n",
    "residual_prop = (decomposition.resid.var() / total_var) * 100\n",
    "\n",
    "print(f\"\\nVariance Decomposition:\")\n",
    "print(f\"Trend: {trend_prop:.1f}%\")\n",
    "print(f\"Seasonal: {seasonal_prop:.1f}%\")\n",
    "print(f\"Residual: {residual_prop:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Basic Forecasting Models\n",
    "\n",
    "### 7.1 Simple Moving Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement basic forecasting methods\n",
    "print(\"BASIC FORECASTING METHODS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Prepare data for forecasting\n",
    "forecast_data = energy_data['EnergyConsumption'].iloc[:1000]  # Use first 1000 points\n",
    "train_size = int(len(forecast_data) * 0.8)\n",
    "train_data = forecast_data[:train_size]\n",
    "test_data = forecast_data[train_size:]\n",
    "\n",
    "print(f\"Training data: {len(train_data)} observations\")\n",
    "print(f\"Test data: {len(test_data)} observations\")\n",
    "\n",
    "# Method 1: Simple Moving Average\n",
    "window_sizes = [12, 24, 48]  # 12, 24, and 48 hours\n",
    "ma_forecasts = {}\n",
    "\n",
    "for window in window_sizes:\n",
    "    # Calculate moving average\n",
    "    ma_forecast = train_data.rolling(window=window).mean().iloc[-1]\n",
    "    ma_forecasts[f'MA_{window}'] = [ma_forecast] * len(test_data)\n",
    "\n",
    "print(f\"\\nMoving Average Forecasts:\")\n",
    "for method, forecast in ma_forecasts.items():\n",
    "    mae = mean_absolute_error(test_data, forecast)\n",
    "    rmse = np.sqrt(mean_squared_error(test_data, forecast))\n",
    "    print(f\"{method}: MAE={mae:.2f}, RMSE={rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Exponential Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential Smoothing models\n",
    "try:\n",
    "    # Simple Exponential Smoothing\n",
    "    ses_model = ExponentialSmoothing(train_data, trend=None, seasonal=None)\n",
    "    ses_fit = ses_model.fit()\n",
    "    ses_forecast = ses_fit.forecast(steps=len(test_data))\n",
    "    \n",
    "    # Double Exponential Smoothing (Holt's method)\n",
    "    des_model = ExponentialSmoothing(train_data, trend='add', seasonal=None)\n",
    "    des_fit = des_model.fit()\n",
    "    des_forecast = des_fit.forecast(steps=len(test_data))\n",
    "    \n",
    "    # Triple Exponential Smoothing (Holt-Winters)\n",
    "    # Use a subset for computational efficiency\n",
    "    daily_train = train_data.resample('H').mean().dropna()[:168]  # One week of data\n",
    "    if len(daily_train) >= 48:  # Need enough data for seasonal component\n",
    "        tes_model = ExponentialSmoothing(daily_train, trend='add', seasonal='add', seasonal_periods=24)\n",
    "        tes_fit = tes_model.fit()\n",
    "        tes_forecast = tes_fit.forecast(steps=24)  # Forecast next 24 hours\n",
    "    else:\n",
    "        tes_forecast = None\n",
    "    \n",
    "    print(f\"\\nExponential Smoothing Results:\")\n",
    "    \n",
    "    # Evaluate Simple Exponential Smoothing\n",
    "    ses_mae = mean_absolute_error(test_data, ses_forecast)\n",
    "    ses_rmse = np.sqrt(mean_squared_error(test_data, ses_forecast))\n",
    "    print(f\"Simple ES: MAE={ses_mae:.2f}, RMSE={ses_rmse:.2f}\")\n",
    "    \n",
    "    # Evaluate Double Exponential Smoothing\n",
    "    des_mae = mean_absolute_error(test_data, des_forecast)\n",
    "    des_rmse = np.sqrt(mean_squared_error(test_data, des_forecast))\n",
    "    print(f\"Double ES: MAE={des_mae:.2f}, RMSE={des_rmse:.2f}\")\n",
    "    \n",
    "    if tes_forecast is not None:\n",
    "        print(f\"Triple ES: Successfully fitted (limited forecast due to data constraints)\")\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Plot actual data\n",
    "    plt.plot(train_data.index[-100:], train_data.iloc[-100:], \n",
    "             label='Training Data', color='blue', linewidth=1.5)\n",
    "    plt.plot(test_data.index, test_data, \n",
    "             label='Actual Test Data', color='black', linewidth=1.5)\n",
    "    \n",
    "    # Plot forecasts\n",
    "    plt.plot(test_data.index, ses_forecast, \n",
    "             label='Simple Exponential Smoothing', color='red', linewidth=1.5, linestyle='--')\n",
    "    plt.plot(test_data.index, des_forecast, \n",
    "             label='Double Exponential Smoothing', color='green', linewidth=1.5, linestyle='--')\n",
    "    \n",
    "    plt.axvline(x=test_data.index[0], color='gray', linestyle=':', alpha=0.7, label='Forecast Start')\n",
    "    plt.title('Energy Consumption Forecasting Comparison')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Energy Consumption (kWh)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in exponential smoothing: {str(e)}\")\n",
    "    print(\"Continuing with other methods...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation and Validation\n",
    "\n",
    "### 8.1 Forecast Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive model evaluation\n",
    "def evaluate_forecast(actual, predicted, model_name):\n",
    "    \"\"\"\n",
    "    Calculate various forecast accuracy metrics\n",
    "    \"\"\"\n",
    "    # Ensure arrays are numpy arrays\n",
    "    actual = np.array(actual)\n",
    "    predicted = np.array(predicted)\n",
    "    \n",
    "    # Basic metrics\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # Percentage metrics\n",
    "    mape = np.mean(np.abs((actual - predicted) / actual)) * 100\n",
    "    \n",
    "    # Theil's U statistic (naive forecast comparison)\n",
    "    naive_forecast = np.full_like(actual, actual[0])  # Use first value as naive forecast\n",
    "    naive_mse = mean_squared_error(actual, naive_forecast)\n",
    "    theil_u = rmse / np.sqrt(naive_mse)\n",
    "    \n",
    "    # Directional accuracy\n",
    "    if len(actual) > 1:\n",
    "        actual_direction = np.diff(actual) > 0\n",
    "        predicted_direction = np.diff(predicted) > 0\n",
    "        directional_accuracy = np.mean(actual_direction == predicted_direction) * 100\n",
    "    else:\n",
    "        directional_accuracy = np.nan\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'MAPE': mape,\n",
    "        'Theil_U': theil_u,\n",
    "        'Directional_Accuracy': directional_accuracy\n",
    "    }\n",
    "\n",
    "# Evaluate all models\n",
    "evaluation_results = []\n",
    "\n",
    "# Moving averages\n",
    "for method, forecast in ma_forecasts.items():\n",
    "    result = evaluate_forecast(test_data, forecast, method)\n",
    "    evaluation_results.append(result)\n",
    "\n",
    "# Exponential smoothing models\n",
    "if 'ses_forecast' in locals():\n",
    "    result = evaluate_forecast(test_data, ses_forecast, 'Simple_ES')\n",
    "    evaluation_results.append(result)\n",
    "\n",
    "if 'des_forecast' in locals():\n",
    "    result = evaluate_forecast(test_data, des_forecast, 'Double_ES')\n",
    "    evaluation_results.append(result)\n",
    "\n",
    "# Create evaluation DataFrame\n",
    "evaluation_df = pd.DataFrame(evaluation_results)\n",
    "\n",
    "print(\"MODEL EVALUATION RESULTS\")\n",
    "print(\"=\" * 40)\n",
    "display(evaluation_df.round(3))\n",
    "\n",
    "# Find best model\n",
    "best_rmse_model = evaluation_df.loc[evaluation_df['RMSE'].idxmin(), 'Model']\n",
    "best_mae_model = evaluation_df.loc[evaluation_df['MAE'].idxmin(), 'Model']\n",
    "\n",
    "print(f\"\\nBest Models:\")\n",
    "print(f\"Lowest RMSE: {best_rmse_model}\")\n",
    "print(f\"Lowest MAE: {best_mae_model}\")\n",
    "\n",
    "# Visualization of evaluation metrics\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# RMSE comparison\n",
    "ax1.bar(evaluation_df['Model'], evaluation_df['RMSE'], color='skyblue', edgecolor='black')\n",
    "ax1.set_title('Root Mean Square Error (RMSE)')\n",
    "ax1.set_ylabel('RMSE')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# MAE comparison\n",
    "ax2.bar(evaluation_df['Model'], evaluation_df['MAE'], color='lightcoral', edgecolor='black')\n",
    "ax2.set_title('Mean Absolute Error (MAE)')\n",
    "ax2.set_ylabel('MAE')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# MAPE comparison\n",
    "ax3.bar(evaluation_df['Model'], evaluation_df['MAPE'], color='lightgreen', edgecolor='black')\n",
    "ax3.set_title('Mean Absolute Percentage Error (MAPE)')\n",
    "ax3.set_ylabel('MAPE (%)')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Directional Accuracy comparison\n",
    "valid_da = evaluation_df.dropna(subset=['Directional_Accuracy'])\n",
    "if not valid_da.empty:\n",
    "    ax4.bar(valid_da['Model'], valid_da['Directional_Accuracy'], color='gold', edgecolor='black')\n",
    "    ax4.set_title('Directional Accuracy')\n",
    "    ax4.set_ylabel('Accuracy (%)')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'Directional Accuracy\\nNot Available', \n",
    "             ha='center', va='center', transform=ax4.transAxes)\n",
    "    ax4.set_xlim(0, 1)\n",
    "    ax4.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Recommendations\n",
    "\n",
    "### 9.1 Key Findings and Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TIME SERIES ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Dataset summary\n",
    "print(f\"\\nDATASET OVERVIEW:\")\n",
    "print(f\"Total observations: {len(energy_data):,}\")\n",
    "print(f\"Time span: {energy_data.index.min()} to {energy_data.index.max()}\")\n",
    "print(f\"Frequency: Hourly\")\n",
    "print(f\"Missing values: {energy_data['EnergyConsumption'].isnull().sum()}\")\n",
    "\n",
    "# Stationarity findings\n",
    "print(f\"\\nSTATIONARITY ANALYSIS:\")\n",
    "print(f\"Energy consumption stationarity: {stationarity_result['conclusion']}\")\n",
    "if 'diff_result' in locals():\n",
    "    print(f\"First difference stationarity: {diff_result['conclusion']}\")\n",
    "\n",
    "# Pattern findings\n",
    "print(f\"\\nSEASONAL PATTERNS DETECTED:\")\n",
    "print(f\"Daily cycle: Peak at {hourly_avg.idxmax()}:00, Low at {hourly_avg.idxmin()}:00\")\n",
    "print(f\"Weekly cycle: Highest on {days[weekly_avg.idxmax()]}, Lowest on {days[weekly_avg.idxmin()]}\")\n",
    "print(f\"Annual cycle: Peak in {months[monthly_avg.idxmax()-1]}, Low in {months[monthly_avg.idxmin()-1]}\")\n",
    "\n",
    "# Best model\n",
    "if not evaluation_df.empty:\n",
    "    print(f\"\\nFORECASTING PERFORMANCE:\")\n",
    "    print(f\"Best RMSE model: {best_rmse_model} (RMSE: {evaluation_df['RMSE'].min():.2f})\")\n",
    "    print(f\"Best MAE model: {best_mae_model} (MAE: {evaluation_df['MAE'].min():.2f})\")\n",
    "    print(f\"Models evaluated: {len(evaluation_df)}\")\n",
    "\n",
    "print(f\"\\nKEY INSIGHTS:\")\n",
    "print(\"1. Energy consumption shows clear daily and seasonal patterns\")\n",
    "print(\"2. Strong autocorrelation suggests predictable behavior\")\n",
    "print(\"3. Temperature and occupancy are key drivers of consumption\")\n",
    "print(\"4. Simple methods can provide reasonable baseline forecasts\")\n",
    "print(\"5. Seasonal decomposition reveals trend and cyclical components\")\n",
    "\n",
    "print(f\"\\nRECOMMENDAT1ONS:\")\n",
    "print(\"• Test stationarity before modeling with ARIMA-type models\")\n",
    "print(\"• Consider multiple seasonalities (daily, weekly, annual)\")\n",
    "print(\"• Include external variables (temperature, occupancy) for better accuracy\")\n",
    "print(\"• Use cross-validation for robust model selection\")\n",
    "print(\"• Monitor forecast performance and update models regularly\")\n",
    "print(\"• Consider ensemble methods for improved robustness\")\n",
    "\n",
    "print(f\"\\nNEXT STEPS:\")\n",
    "print(\"• Implement ARIMA/SARIMA models for more sophisticated forecasting\")\n",
    "print(\"• Explore machine learning approaches (Random Forest, XGBoost)\")\n",
    "print(\"• Implement deep learning models (LSTM, CNN) for complex patterns\")\n",
    "print(\"• Develop multivariate models incorporating external factors\")\n",
    "print(\"• Create automated forecasting pipeline with model monitoring\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}